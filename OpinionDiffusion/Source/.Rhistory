source('D:/WSL/IdentifyingNarratives/final.R')
#Main function which takes search term, file name and cluster number as parameters and writes the output into xlsx file
final_clusters <- function(search_term,file_count,method,n){
#search_term="demonetization"
#search_term="womensmarch"
#file_count="demonetization-tweets"
#file_count="womenmarch"
#clust_count=8
#method="weighted"
# foldername=paste0(file_count,"-",Sys.time())
foldername = file_count
if(!dir.exists(foldername)){
dir.create(foldername)
}
cluster_count <- Part1(search_term,n,paste0(file_count,"_","Clusters",".csv"),file_count,foldername)
#cluster_count=clust_count
print("Starting abstraction and expression extraction process!")
print(paste0("number of clusters : ",cluster_count))
file_name <- paste0(file_count,"_","Clusters",".csv")
#cleaning#
#----------------#
all_Tweets <- read.csv(paste0(foldername,"/",file_name))
#removing non-ascii characters and emoticons
all_Tweets$cleantweet <- gsub("[^\x01-\x7F]", " ", all_Tweets$tweet)
all_Tweets$cleantweet <- gsub("<\\w+ *>", " ", all_Tweets$cleantweet)
#removing URLS
all_Tweets$cleantweet <- gsub("http\\S+\\s*", " ", all_Tweets$cleantweet)
all_Tweets$cleantweet <- gsub("htt", " ", all_Tweets$cleantweet)
all_Tweets$cleantweet <- gsub("http", " ", all_Tweets$cleantweet)
#remove user mentions
all_Tweets$cleantweet <- gsub("@\\w+", " ", all_Tweets$cleantweet)
#removing amp symbol
all_Tweets$cleantweet <- gsub("&amp", " ", all_Tweets$cleantweet)
#removing rt header from tweet
#all_Tweets$cleantweet <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)"," ",all_Tweets$cleantweet)
#all_Tweets$cleantweet <- gsub("(rt|via)((?:\\b\\W*@\\w+)+)"," ",all_Tweets$cleantweet)
#removing punctuation
all_Tweets$cleantweet <- gsub("[[:punct:]]+", " ", all_Tweets$cleantweet)
#conversion to lower case
all_Tweets$cleantweet <- tolower(all_Tweets$cleantweet)
#remove 'rt' in the beginning of tweet
all_Tweets$cleantweet <- gsub("^rt\\s", " ", all_Tweets$cleantweet)
#removing whitespaces or multiple spaces
all_Tweets$cleantweet <- gsub("^\\s+|\\s+$", " ", all_Tweets$cleantweet)
all_Tweets$cleantweet <- clean(all_Tweets$cleantweet)
#Creating a data frame which will contain data of all tweets.
df <- data.frame("cl_num"= all_Tweets$cl_num,"original_tweet"=all_Tweets$tweet,"cleaned_tweet"=all_Tweets$cleantweet,"sentiment"=all_Tweets$sentiment,"dist"=all_Tweets$dist,"freq"=all_Tweets$freq)
#Assigning polarity based on the sentiment for each tweet 2=negative, 1=positive, 0=neutral
df$polarity <- NA
df$polarity[which(df$sentiment < 0)] <- "2"
df$polarity[which(df$sentiment > 0)] <- "1"
df$polarity[which(df$sentiment == 0)] <- "0"
#Creating a data frame to store all phrases and calculation related to phrases
cluster_opinions=data.frame(cluster_num=integer(),Abstraction=character(),Term_freq=numeric(),Term_freq_weighted=numeric(),Doc_freq=numeric(),Tf_idf=numeric(),Tf_idf_weighted=numeric(),stringsAsFactors=FALSE)
#First Iteration over all clusters
#In this iteration, identify phrases and their term frequencies for all clusters. And store them in "cluster_opinions" data frame we just created
for(i in 1:cluster_count){
#Get tweets for one cluster
cluster <- df[df$cl_num == i,]
#Dump all the cleaned tweets into a variable,separated by ". "
tweet_words=paste( unlist(cluster$cleaned_tweet), collapse='. ')
#Call extractChunks function and store all extracted phrases
extracted_words=extractChunks(tweet_words)
#If no phrases are extracted, skip this cluster. We are not including such clusters in result/output
if(length(extracted_words)==0){
print(paste0("No extracted words in cluster ",i))
next()
}
#store all unique phrases identified
all_phrases=rownames(table(extracted_words))
#store phrases whose length is al teast 3
all_phrases=all_phrases[which(nchar(all_phrases)>2)]
#Remove the phrases which are exactly search term ie "demonetisation"
all_phrases=all_phrases[which(all_phrases!=search_term)]
#arrange all phrases in ascending order of length of each phrase
all_phrases=all_phrases[order(nchar(all_phrases))]
num_phrases=length(all_phrases)
all_phrases_new=rep("",0)
Term_freq=rep(0,0)
Term_freq_weighted=rep(0,0)
#Loop to go through each phrase extracted
#there will be multiple phrases which are different versions of same phrase. For example there will be three different phrases like
# 1) "Inauguration riots" 2) "Inauguration riots in Washington" 3) "Inauguration riots in DC"
#We don't want three versions, we only want one phrase which is repeated the most.
#To eliminate this, for each phrase, get all the phrases which contain this phrase, replace all these phrases with the phrase which occurs in maximum number of tweets
#Store all these new phrases in a new variable
for(k in 1:num_phrases){
ph=all_phrases[k]
if(length(grep(paste0("^",ph,"$"),all_phrases_new,fixed = FALSE))>0){
#phrase already exists
#ignore and go to next phrase
#print(paste0(k," phrase already taken",ph))
next()
}
#Get all phrases which contain this phrase
dup=grep(ph,all_phrases,fixed = TRUE)
if(length(dup)==1){
#no repetitions, add phrase to new variable
#update term frequency
#print(paste0(k," only one present ",ph))
all_phrases_new[length(all_phrases_new)+1]=ph
Term_freq[length(Term_freq)+1]=table(extracted_words)[[ph]]
tweets=cluster[grep(ph,cluster$cleaned_tweet),]
Term_freq_weighted[length(Term_freq_weighted)+1]=sum(tweets$freq)
next()
}
#If there are multiple duplicates, get frequency of all phrases
dup_count=rep(0,length(dup))
for(w in 1:length(dup)){
dup_count[w] = nrow(cluster[grep(all_phrases[dup[w]],cluster$cleaned_tweet),])
}
#Pick the word which occurs in maximum tweets
final_word=all_phrases[dup[which.max(dup_count)[1]]]
#Replace all duplicate phrases with selected phrase, so that loop won't go through it again
all_phrases=replace(all_phrases,dup,final_word)
#add the final selected word to new variable and add term frequency
all_phrases_new[length(all_phrases_new)+1]=final_word
Term_freq[length(Term_freq)+1]=max(dup_count)
#find weighted term frequency
tweets=cluster[grep(final_word,cluster$cleaned_tweet),]
Term_freq_weighted[length(Term_freq_weighted)+1]=sum(tweets$freq)
#print(paste0(k," phrase is",ph," final word is ",final_word))
}
#Add clnum,abstraction,termfreq to "cluster_opinions" data frame, which contains data about all phrases
#Add a list of '0's to doc_freq and tf_idf
cluster_num=rep(i,length(all_phrases_new))
Doc_freq=rep(0,length(all_phrases_new))
Tf_idf=rep(0,length(all_phrases_new))
Tf_idf_weighted=rep(0,length(all_phrases_new))
Abstraction=all_phrases_new
temp_df=cbind(cluster_num,Abstraction,Term_freq,Term_freq_weighted,Doc_freq,Tf_idf,Tf_idf_weighted,stringsAsFactors =FALSE)
cluster_opinions=rbind(cluster_opinions,temp_df,stringsAsFactors =FALSE)
}
print("Extracting all opinion done! Starting filtering opinions")
#For all phrases of all clusters, calculate document frequency and tf-idf
for(j in 1:nrow(cluster_opinions)){
#find number of clusters that contain this phrase
#nclus=as.numeric(cluster_opinions[cluster_opinions$Abstraction== as.character(cluster_opinions$Abstraction[[j]]),]$cluster_num)
nclus=cluster_opinions[grep(as.character(cluster_opinions$Abstraction[[j]]),cluster_opinions$Abstraction),]$cluster_num
#idf for all phrases that occur in only one cluster is log(total number of clusters/1)=log(total number of clusters)
std_idf = log(cluster_count)
#if it occurs in only one cluster
if(length(nclus)==1){
#Replace document frequent and tf-idf in "cluster_opinions" data frame
cluster_opinions$Doc_freq[j]=1
cluster_opinions$Tf_idf[j]=(as.numeric(cluster_opinions$Term_freq[j])*std_idf)
cluster_opinions$Tf_idf_weighted[j]=(as.numeric(cluster_opinions$Term_freq_weighted[j])*std_idf)
}
else{
#If phrase occurs in many clusters, Idf=log(total number of clusters/ number of clusters phrase occurred in)
doc_f=length(unique(nclus))
idf=log(cluster_count/doc_f)
#Replace document frequent and tf-idf in "cluster_opinions" data frame
cluster_opinions$Doc_freq[j]=doc_f
cluster_opinions$Tf_idf[j]=(as.numeric(cluster_opinions$Term_freq[j])*idf)
cluster_opinions$Tf_idf_weighted[j]=(as.numeric(cluster_opinions$Term_freq_weighted[j])*idf)
}
}
#final iteration to Find final top abstraction (phrases) and respective expression (sentiment)
#create workbook for writing results in sheets
wb = createWorkbook()
#Loop through each cluster
for(j in 1:cluster_count){
#Get all phrases for the cluster
extracted<-cluster_opinions[cluster_opinions$cluster_num==j,]
ind=0
#order phrases by descending order of Tf-idf score
#Get the index where the phrases have to be divided by passing the tf-idf distribution to "Find_Top_Abstractions" function
if(method=="simple"){
extracted<-extracted[order(-as.numeric(extracted$Tf_idf)),]
ind<-Find_Top_Abstractions(as.numeric(extracted$Tf_idf),nrow(extracted))
}else if(method=="weighted"){
extracted<-extracted[order(-as.numeric(extracted$Tf_idf_weighted)),]
ind<-Find_Top_Abstractions(as.numeric(extracted$Tf_idf_weighted),nrow(extracted))
}
#all words from 1 to index returned are considered as final selected phrases
selected_abstraction<-extracted$Abstraction[1:ind]
#Get all tweets for this cluster number
cluster_tweets <- df[df$cl_num == j,]
num_phrases<-length(selected_abstraction)
#empty integer list of length 3. 1st,2nd and 3rd position for storing neutral tweets,positive tweets and negative tweets respectively
phrase_senti_individual=integer(3)
#Empty list of list of length as many as selected phrases. [[000],[000],[000]] First element is for first phrase, second for second phrase etc
phrase_senti=rep(list(phrase_senti_individual),num_phrases)
#Character list to store final sentiment for all phrases
phrase_final_senti=character(num_phrases)
print("Extracting abstraction done! Starting finding respective sentiment.")
#Loop through each selected phrase and find most occurring sentiment for the phrase
for(k in 1: num_phrases){
ph=selected_abstraction[k]
#Get all tweets containing this phrase
tweets=cluster_tweets[grep(ph,cluster_tweets$cleaned_tweet),]
if(nrow(tweets)>0){
#Loop through all tweets
for(l in 1:nrow(tweets)){
#If polarity of tweet is "0", it is neutral =1. polarity 1 is positive and polarity 2 is negative
if(tweets$polarity[l]==0){
#If polarity of 1st phrase is "0", add 1 to phrase_senti[1,1] which is first position of first list.
#print("neutral")
phrase_senti[[k]][1]=phrase_senti[[k]][1] + 1
}else if(tweets$polarity[l]==1){
#print("positive")
phrase_senti[[k]][2]=phrase_senti[[k]][2] + 1
}else if(tweets$polarity[l]==2){
#print("negative")
phrase_senti[[k]][3]=phrase_senti[[k]][3] + 1
}
}
}
}
#Loop through each phrase to find maximum occurring sentiment
for(k in 1:num_phrases){
#which position does the maximum sentiment occur? If its 1st position, most repeated sentiment is "neutral"
mode_senti=which.max(phrase_senti[[k]])
#print(paste0(" senti index value is ", mode_senti))
if(mode_senti==1){
phrase_final_senti[k]="Neutral"
} else if(mode_senti==2){
phrase_final_senti[k]="Positive"
} else if(mode_senti==3){
phrase_final_senti[k]="Negative"
}
}
#selected_abstraction
#phrase_final_senti
print("Writing results into file!")
#For this cluster, Write results into xlsx sheet
#xlsx sheet should have equal rows in all columns
if(nrow(cluster_tweets)>num_phrases){
#If number of tweets are more than number of phrases
#Fill the empty phrases with empty string until number of rows become equal to number of tweets
selected_abstraction[(num_phrases+1):nrow(cluster_tweets)] <- ""
phrase_final_senti[(num_phrases+1):nrow(cluster_tweets)] <- ""
#Write in to the sheet
df_helper <- data.frame("Tweet"=cluster_tweets$original_tweet[order(-cluster_tweets$freq)],"Tweet_Count"=cluster_tweets$freq[order(-cluster_tweets$freq)],"Abstraction"=selected_abstraction,"Expression"=phrase_final_senti)
sheet = createSheet(wb, paste0("Narrative ",j))
addDataFrame(df_helper, sheet=sheet, startColumn = 1, row.names=FALSE)
}else if(nrow(cluster_tweets)<num_phrases){
#If number of phrases are more than number of tweets
#Fill the empty tweets with empty string until number of rows become equal to number of phrases
tweet_inter=as.character(cluster_tweets$original_tweet[order(-cluster_tweets$freq)])
tweet_inter[(length(tweet_inter)+1):num_phrases]<-""
count_inter=as.character(cluster_tweets$freq[order(-cluster_tweets$freq)])
count_inter[(length(count_inter)+1):num_phrases]<-""
#Write in to the sheet
df_helper <- data.frame("Tweet"=tweet_inter,"Tweet_Count"=count_inter,"Abstraction"=selected_abstraction,"Expression"=phrase_final_senti)
sheet = createSheet(wb, paste0("Narrative ",j))
addDataFrame(df_helper, sheet=sheet, startColumn = 1, row.names=FALSE)
}else{
#If number of phrases == number of tweets, no need of filling, just write it to sheet
df_helper <- data.frame("Tweet"=cluster_tweets$original_tweet[order(-cluster_tweets$freq)],"Tweet_Count"=cluster_tweets$freq[order(-cluster_tweets$freq)],"Abstraction"=selected_abstraction,"Expression"=phrase_final_senti)
sheet = createSheet(wb, paste0("Narrative ",j))
addDataFrame(df_helper, sheet=sheet, startColumn = 1, row.names=FALSE)
}
}
#Save the file
#print("writing final file")
saveWorkbook(wb, paste0(foldername,"/",file_count,"_final", ".xlsx"))
print("Process complete!")
#function end
}
setwd("D:/WSL/SCM/Cogno/Source")
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
View(nodes)
source('D:/WSL/SCM/Cogno/Source/dominance.R')
View(edges)
source('D:/WSL/SCM/Cogno/Source/dominance.R')
View(edges)
View(edges)
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
View(direction_matrix)
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/cluster_wise_user_analysis.R')
source('D:/WSL/SCM/Cogno/Source/cluster_wise_user_analysis.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
source('D:/WSL/SCM/Cogno/Source/dominance.R')
